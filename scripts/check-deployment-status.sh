#!/bin/bash

echo "=========================================="
echo "ğŸ“Š Ray Service Deployment Status"
echo "=========================================="
echo ""

echo "ğŸ–¥ï¸  Nodes:"
kubectl get nodes -o wide
echo ""

echo "ğŸ“¦ Pods:"
kubectl get pods -l ray.io/cluster=vllm-serve -o wide
echo ""
kubectl get pods -l app=redis
echo ""

echo "ğŸ¯ Ray Cluster Status:"
kubectl exec -it vllm-serve-raycluster-bhr9w-head-h8xw8 -- ray status 2>/dev/null || echo "Head pod not ready"
echo ""

echo "ğŸš€ Ray Serve Status:"
kubectl exec -it vllm-serve-raycluster-bhr9w-head-h8xw8 -- serve status 2>/dev/null || echo "Serve not ready"
echo ""

echo "ğŸ›¡ï¸  Pod Disruption Budgets:"
kubectl get pdb
echo ""

echo "ğŸŒ Services:"
kubectl get svc -l ray.io/cluster=vllm-serve
echo ""

echo "ğŸ“Š RayService Status:"
kubectl get rayservice vllm-serve
echo ""

echo "=========================================="
echo "To test inference once ready, run:"
echo "kubectl run test-inference --rm -it --restart=Never --image=curlimages/curl:latest -- curl -X POST http://vllm-serve-head-svc:8000/VLLMDeployment -H 'Content-Type: application/json' -d '{\"prompt\": \"What is the capital of France?\", \"max_tokens\": 100}'"
echo "=========================================="
