---
# Ray Service with vLLM v0.11.0 using Ray Serve LLM API and RunAI Streamer
apiVersion: ray.io/v1
kind: RayService
metadata:
  name: vllm-serve
  namespace: default
spec:
  serviceUnhealthySecondThreshold: 900
  deploymentUnhealthySecondThreshold: 900
  serveConfigV2: |
    applications:
    - name: vllm-app
      import_path: ray.serve.llm:build_openai_app
      runtime_env:
        pip:
          - numpy==1.26.4
          - pandas==2.2.3
          - vllm[runai]==0.11.0
        env_vars:
          VLLM_USE_V1: "1"
          AWS_DEFAULT_REGION: "us-west-2"
          AWS_REGION: "us-west-2"
      args:
        llm_configs:
          - model_loading_config:
              model_id: qwen-0.5b
              model_source: /s3/models/Qwen/Qwen2.5-0.5B-Instruct
            accelerator_type: L4
            deployment_config:
              autoscaling_config:
                min_replicas: 3
                max_replicas: 3
              max_ongoing_requests: 100
            engine_kwargs:
              load_format: runai_streamer
              tensor_parallel_size: 1
              gpu_memory_utilization: 0.9
              max_model_len: 2048
              trust_remote_code: true
              enable_chunked_prefill: false
  rayClusterConfig:
    rayVersion: '2.51.0'
    enableInTreeAutoscaling: false
    gcsFaultToleranceOptions:
      redisAddress: "ray-redis-cluster.dd32wx.ng.0001.usw2.cache.amazonaws.com:6379"
      externalStorageNamespace: "vllm-serve"
    headGroupSpec:
      rayStartParams:
        dashboard-host: '0.0.0.0'
        num-cpus: '0'
        metrics-export-port: '8080'
      serviceType: ClusterIP
      template:
        spec:
          # Ensure head node is on a separate node from workers
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: ray.io/group-name
                      operator: In
                      values:
                      - gpu-workers
                  topologyKey: kubernetes.io/hostname
          containers:
          - name: ray-head
            image: ${AWS_ACCOUNT_ID}.dkr.ecr.us-west-2.amazonaws.com/ray-serve:2.51.0-py311-gpu
            ports:
            - containerPort: 6379
              name: gcs
            - containerPort: 8265
              name: dashboard
            - containerPort: 10001
              name: client
            - containerPort: 8000
              name: serve
            - containerPort: 8080
              name: metrics
            resources:
              requests:
                cpu: "2"
                memory: "8Gi"
              limits:
                cpu: "4"
                memory: "16Gi"
            # Add health checks for the head node
            livenessProbe:
              httpGet:
                path: /api/serve/applications/
                port: 8265
              initialDelaySeconds: 60
              periodSeconds: 30
              timeoutSeconds: 10
              failureThreshold: 5
            readinessProbe:
              httpGet:
                path: /api/serve/applications/
                port: 8265
              initialDelaySeconds: 30
              periodSeconds: 10
              timeoutSeconds: 5
              failureThreshold: 3
            # Graceful shutdown
            lifecycle:
              preStop:
                exec:
                  command:
                  - /bin/sh
                  - -c
                  - |
                    # Wait for ongoing requests to complete
                    sleep 30
            env:
            - name: RAY_external_storage_namespace
              value: "vllm-serve"
            - name: RAY_gcs_rpc_server_reconnect_timeout_s
              value: "300"
          terminationGracePeriodSeconds: 60
    workerGroupSpecs:
    - replicas: 3
      minReplicas: 3
      maxReplicas: 3
      groupName: gpu-workers
      rayStartParams:
        num-cpus: '4'
        num-gpus: '1'
        metrics-export-port: '8080'
      template:
        spec:
          nodeSelector:
            karpenter.sh/nodepool: gpu-nodepool
          tolerations:
          - key: nvidia.com/gpu
            operator: Exists
            effect: NoSchedule
          # Spread workers across different nodes
          topologySpreadConstraints:
          - maxSkew: 1
            topologyKey: kubernetes.io/hostname
            whenUnsatisfiable: DoNotSchedule
            labelSelector:
              matchLabels:
                ray.io/group: gpu-workers
          # Anti-affinity to avoid multiple workers on same node
          affinity:
            podAntiAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
              - labelSelector:
                  matchExpressions:
                  - key: ray.io/group
                    operator: In
                    values:
                    - gpu-workers
                topologyKey: kubernetes.io/hostname
          containers:
          - name: ray-worker
            image: ${AWS_ACCOUNT_ID}.dkr.ecr.us-west-2.amazonaws.com/ray-serve:2.51.0-py311-gpu
            resources:
              requests:
                cpu: "4"
                memory: "16Gi"
                nvidia.com/gpu: "1"
              limits:
                nvidia.com/gpu: "1"
            # Add health checks for workers
            livenessProbe:
              exec:
                command:
                - /bin/sh
                - -c
                - ray health-check || exit 1
              initialDelaySeconds: 60
              periodSeconds: 30
              timeoutSeconds: 10
              failureThreshold: 5
            readinessProbe:
              exec:
                command:
                - /bin/sh
                - -c
                - ray health-check || exit 1
              initialDelaySeconds: 60
              periodSeconds: 15
              timeoutSeconds: 10
              failureThreshold: 5
            # Graceful shutdown for workers
            lifecycle:
              preStop:
                exec:
                  command:
                  - /bin/sh
                  - -c
                  - |
                    # Drain the worker gracefully
                    ray stop --grace-period 30
            env:
            - name: RAY_external_storage_namespace
              value: "vllm-serve"
            - name: RAY_gcs_rpc_server_reconnect_timeout_s
              value: "300"
            volumeMounts:
            - name: s3-storage
              mountPath: /s3
              readOnly: true
            - name: shm
              mountPath: /dev/shm
          terminationGracePeriodSeconds: 60
          volumes:
          - name: s3-storage
            persistentVolumeClaim:
              claimName: s3-pvc
          - name: shm
            emptyDir:
              medium: Memory
              sizeLimit: 10Gi
---
# PDB for Ray head node
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: vllm-serve-head-pdb
  namespace: default
spec:
  minAvailable: 1
  selector:
    matchLabels:
      ray.io/node-type: head
      ray.io/serve: "true"
---
# PDB for Ray workers - require at least 2 available during disruptions
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: vllm-serve-worker-pdb
  namespace: default
spec:
  minAvailable: 2
  selector:
    matchLabels:
      ray.io/node-type: worker
      ray.io/serve: "true"
